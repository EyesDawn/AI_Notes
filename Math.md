# 1 Normalization

- 计算每个样本的均值 μ 和标准差 σ： 
  $$
  \begin{aligned}&\mu_x=\frac1T\sum_{t=1}^Tx_t\\&\sigma_x=\sqrt{\frac1T\sum_{t=1}^T(x_t-\mu_x)^2+\epsilon}\end{aligned}
  $$
  
- 进行归一化：

$$
x^{\prime}=\frac{x-\mu_x}{\sigma_x}
$$



- 归一化后的数据用于输入深度学习模型进行训练或推理。

**点积与余弦相似度的联系**

若特征向量 $\mathbf{r}_{i,t}$ 和 $\mathbf{r}'_{i,t}$ 被归一化（即 $\|\mathbf{r}_{i,t}\| = \|\mathbf{r}'_{i,t}\| = 1$），则点积等价于余弦相似度：
$$
\mathbf{r}_{i,t} \cdot \mathbf{r}'_{i,t} = \|\mathbf{r}_{i,t}\| \|\mathbf{r}'_{i,t}\| \cos\theta = \cos\theta.
$$

## 1.1 为什么时间序列预测需要归一化？

时间序列数据通常具有**非平稳性（Non-Stationarity）**，即其**均值、方差随时间变化**。传统神经网络（如 RNN、LSTM、Transformer）在训练过程中，如果输入数据的分布变化较大，可能会导致：

1. **模型难以学习稳定的模式**，因为输入数据的尺度变化大。
2. **梯度更新不稳定**，影响训练收敛效果。

Instance Normalization（IN）在时间序列预测中的应用主要是：

- **消除输入数据的尺度差异**，让模型关注模式，而不是绝对数值。
- **对每个时间序列样本单独进行归一化**，不会受其他样本的影响。
- **增强模型对不同时间序列的泛化能力**，适用于多种数据分布。

## 1.2 IN vs RevIN

| 方面                            | Instance Normalization（IN）                     | Reversible Instance Normalization（RevIN）                 |
| ------------------------------- | ------------------------------------------------ | ---------------------------------------------------------- |
| **归一化目的**                  | 主要用于消除局部统计信息的影响，强调特征的不变性 | 主要用于减少时间序列的非平稳性，提高模型预测稳定性         |
| **适用领域**                    | 风格迁移、GAN、计算机视觉                        | 时间序列预测（TSF）                                        |
| **归一化对象**                  | **对 CNN 每个样本的每个通道独立归一化**          | **对时间序列的每个样本的每个特征独立归一化**               |
| **去均值（Mean Shift）**        | **始终去均值**，输出是零均值数据                 | **可以选择是否去均值**（通过参数控制），保留一定的尺度信息 |
| **归一化后信息损失**            | **会损失原始样本的统计信息**，适用于生成式任务   | **保留一定的统计信息**，能够在推理时恢复原始尺度           |
| **反归一化（Denormalization）** | **无逆变换过程，归一化后的数据直接用于预测**     | **推理后需要逆变换**，恢复到原始尺度，适用于时间序列预测   |
| **能否提高模型泛化性**          | **增强风格不变性，但可能影响内容信息**           | **增强时间序列模型的稳定性和泛化能力**                     |

为什么 RevIN 只能表达基本趋势，而无法处理季节性模式？

**结论：** RevIN 主要通过 **归一化（Normalization）** 和 **反归一化（Denormalization）** 来减少时间序列的非平稳性（Non-Stationarity），从而提高模型的稳定性。然而，它的归一化策略主要针对**均值和标准差的调整**，这使得 RevIN **只能捕捉全局趋势，而无法建模复杂的周期性（Seasonality）模式**。

- 时间序列可以分解为**趋势（Trend）+季节性（Seasonality）+随机噪声（Noise）**。
- **傅里叶变换（Fourier Transform）** 和 **小波变换（Wavelet Transform）** 可以提取时间序列的周期性信息。
- 滑动窗口归一化：仅对短时间窗口内的数据归一化，保持局部结构。

## 1.3 多维正态分布

多维正态分布（Multivariate Normal Distribution）与一维正态分布（Univariate Normal Distribution）的核心区别在于**维度的扩展**，以及由此带来的统计性质和数学表达上的差异。以下是两者的主要区别：

---

### **1. 定义与数学表达**

- **一维正态分布**  

  - 描述单个随机变量的分布，数学形式为：  
    $$
    N(x; \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
    $$

  - **参数**：标量均值 $\mu$ 和标量方差 $\sigma^2$。

- **多维正态分布**  

  - 描述多个随机变量（向量）的联合分布，数学形式为：  
    $$
    N(\mathbf{x}; \boldsymbol{\mu}, \Sigma) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x}-\boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)
    $$

  - **参数**：均值向量 $\boldsymbol{\mu}$（描述各维度的中心位置）和协方差矩阵 $\Sigma$（描述维度间的相关性及各自的方差）。

---

### **2. 统计性质**

- **一维正态分布**  
  - 仅需均值和方差即可完全确定分布形状。  
  - 概率密度函数（PDF）是对称的钟形曲线（单峰）。

- **多维正态分布**  
  - 需要协方差矩阵 $\Sigma$ 描述变量间的**相关性**（协方差）和各自的**方差**（对角线元素）。  
  - 概率密度函数在空间中呈现“钟形曲面”（如二维时为钟形山丘，三维及以上为超曲面）。  
  - 协方差矩阵决定了分布的**形状**：  
    - 若协方差矩阵是对角矩阵，各维度独立；  
    - 非对角协方差矩阵表示维度间存在线性相关性（如椭圆的长轴方向）。

---

### **3. 几何特征**

- **一维正态分布**  
  - 分布由均值和标准差完全确定，标准差控制钟形曲线的宽度。

- **多维正态分布**  
  - 均值向量 $\boldsymbol{\mu}$ 决定分布的“中心点”。  
  - 协方差矩阵 $\Sigma$ 决定分布的**方向**和**拉伸程度**：  
    - 特征值决定各主轴方向的方差大小；  
    - 特征向量决定主轴的方向。  
  - 例如，二维正态分布的等高线是椭圆，三维时为椭球，高维时为超椭球。

---

### **4. 协方差矩阵的作用**

- **一维正态分布**中，方差 $\sigma^2$ 仅表示数据的离散程度。  
- **多维正态分布**中，协方差矩阵 $\Sigma$ 包含更多信息：  
  - 对角线元素 $\Sigma_{ii}$ 是第 $i$ 个变量的方差；  
  - 非对角线元素 $\Sigma_{ij}$（$i \neq j$）是变量 $i$ 和变量 $j$ 的协方差，反映它们的线性相关性。

---

总结来说，**多维正态分布是一维正态分布在更高维度的推广**，通过协方差矩阵引入变量间的相关性，能够更全面地描述复杂系统的联合分布特性。

# 2 傅里叶变换

- 傅里叶变换让我们输入一个事物，并将其分解为不同频率的成分；
- 频率告诉我们有关数据的一些基本属性；

- 并且可以通过仅存储重要的成分来压缩数据；

# 3 Fundamental Conception

## 3.1 Covariate and Autocorelation

### Independent Variable vs Covariate

| 特性         | **Independent Variable（独立变量）**       | **Covariate（协变量）**                        |
| ------------ | ------------------------------------------ | ---------------------------------------------- |
| **作用**     | 主要解释或预测因变量，核心研究因素         | 控制干扰因素，防止混杂影响                     |
| **是否操控** | **可以人为操控**（如实验设计中的处理变量） | 通常无法操控，作为**背景特征**或**已存在因素** |
| **分析目的** | 研究它如何影响因变量                       | 调整、平衡它们，排除其他干扰因素的影响         |
| **举例**     | 药物剂量、教育方法、广告曝光量             | 年龄、性别、地区、家庭收入等背景因素           |
| **统计模型** | 回归模型中的主要解释变量（如 X）           | 作为控制变量加入模型，减少误差（如 Z）         |

**协变量分布一致性（Consistent Covariate Distribution）**

- 如果在不同的数据集（如训练集和测试集）中，协变量的分布相似或相同，就称为具有**一致的协变量分布**。
- 举例：如果训练模型时的数据主要来自城市，但测试数据来自农村，模型可能无法有效预测，因为城乡特征分布不同。

### 自相关性

- **无自相关性**：白噪声的当前值和过去值之间没有统计关系。
- **自相关与时间无关**：只要时间间隔固定，相关性不会随时间变化。

自相关系数和协方差是两个不同的概念，尽管它们都用于衡量变量之间的关系，但计算方法和应用场景有所不同。

#### 1) 协方差（Covariance）
协方差衡量的是两个随机变量之间的线性关系。其计算公式为：

$$
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]
$$
其中，$E[X]$ 和 $E[Y]$ 分别是 $X$ 和 $Y$ 的期望值。协方差的值可以为正、负或零：
- 正值表示 $X$ 和 $Y$ 同向变化；
- 负值表示 $X$ 和 $Y$ 反向变化；
- 零表示 $X$ 和 $Y$ 无线性关系。

#### 2) 自相关系数（Autocorrelation Coefficient）
自相关系数衡量的是时间序列中某一时刻的值与另一时刻的值之间的线性关系。其计算公式为：

$$
\rho_k = \frac{\text{Cov}(X_t, X_{t-k})}{\sqrt{\text{Var}(X_t) \cdot \text{Var}(X_{t-k})}}
$$

其中，$Cov(X_t, X_{t-k})$ 是时间序列在时刻 t 和 t-k 的协方差，$Var(X_t)$ 和 $Var(X_{t-k})$ 分别是时间序列在时刻 t 和 t-k 的方差。自相关系数的取值范围在 -1 到 1 之间：
- 1 表示完全正相关；
- -1 表示完全负相关；
- 0 表示无相关性。

#### 区别
- **协方差**衡量的是两个不同变量之间的关系；
- **自相关系数**衡量的是同一时间序列在不同时间点的关系。

## 3.2 一些定理

### 1) 中心极限定理

中心极限定理（Central Limit Theorem, CLT）指出，**无论总体分布如何**，只要样本量足够大，样本均值的分布将近似服从**正态分布**。具体来说：

1.  **前提条件**：样本独立且来自同一分布，样本量较大（通常 $ n \geq 30 $）。
2.  **结论**：样本均值的分布近似正态，其均值等于总体均值，方差为总体方差除以样本量（$ \sigma^2 / n $）。

**公式**：
$$
\frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim N(0, 1)
$$
其中，$ \bar{X} $ 是样本均值，$ \mu $ 是总体均值，$ \sigma $ 是总体标准差，$ n $ 是样本量。

**意义**：

- 即使总体分布未知，样本均值的分布仍可近似为正态分布。
- 为统计推断（如假设检验、置信区间）提供了理论基础。

**总结**：中心极限定理说明，大样本下，样本均值的分布趋近于正态分布，便于统计分析。

## 3.3 L2 norm

L2 范数（L2 norm）是向量空间中长度的度量，也称为欧几里得范数（Euclidean norm）。它的核心含义是计算一个向量在几何空间中的“直线距离”。

---

1. **数学定义**
对于一个 $ n $ 维向量 $ \mathbf{x} = (x_1, x_2, \dots, x_n) $，L2 范数的计算公式为：
$$
\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$
即所有分量的平方和开平方根。

---

2. **几何意义**
• 向量的长度：L2 范数表示从原点（零向量）到该点的欧几里得距离。  

  例如：二维向量 $ (3, 4) $ 的 L2 范数是 $ \sqrt{3^2 + 4^2} = 5 $，即它在平面坐标系中到原点的距离为 5。

---

3. **机器学习和优化中的意义**
在机器学习中，L2 范数常用于：
• 正则化（Regularization）：在损失函数中加入 L2 范数惩罚项（即 L2 正则化），迫使模型参数 $ \theta $ 趋向较小的值，防止过拟合。  

  例如：损失函数变为：
$$
  \text{Loss} = \text{原始损失} + \lambda \cdot \|\theta\|_2^2
$$
  其中 $ \lambda $ 是正则化强度系数。

• 衡量误差：L2 范数可以表示预测值与真实值之间的差距（如均方误差 MSE）。


---

4. **L2 范数的特性**
• 平滑性：L2 范数是可导的（没有尖峰），适合基于梯度的优化算法（如梯度下降）。  

• 各向同性：它对所有方向的分量平等对待，不会偏好某些维度的稀疏性（与 L1 范数不同）。


---

5. **对比 L1 范数**
• L1 范数：定义为绝对值之和 $ \|\mathbf{x}\|_1 = |x_1| + |x_2| + \cdots + |x_n| $，常用于稀疏化参数（使部分参数为零）。  

• L2 范数：倾向于让参数接近零但不完全为零，适合需要平滑解的场景。


---

一句话总结
L2 范数衡量向量的“几何长度”，在机器学习中用于正则化、衡量误差，并因其平滑性成为优化问题的常用工具。

# 4 Maximum Likelihood Estimation
1. **期望的定义**  
对于任何函数 $f(x)$，如果 $x$ 服从分布 $P_{data}(x)$，则其期望可以表示为：  
$$
E_{x \sim P_{\text{data}}} [f(x)] = \int_{x} P_{\text{data}}(x) f(x) dx
$$
如果是离散变量，则积分换成求和符号 $\sum$。

---

2. **应用到原式**  
在原式中，函数 $f(x)$ 是 $\log P_\theta(x)$，因此：  
$$
E_{x \sim P_{\text{data}}} [\log P_\theta(x)] = \int_{x} P_{\text{data}}(x) \log P_\theta(x) dx
$$
左右两边只是符号的差异，数学含义完全一致。

---

3. **为什么优化目标相同？**  
等式两边的 $\arg\max_\theta$ 表示我们要找到参数 $\theta$，使得左边的期望值最大，或者右边的积分值最大。由于左右两边是等价的表达式，优化的是同一个量，因此：  
$$
\arg\max_\theta \text{（左边）} = \arg\max_\theta \text{（右边）}
$$

---

4. **直观理解**  
• 左边：从真实数据分布 $P_{data}$ 中采样 $x$，计算模型 $P_\theta(x)$ 的对数似然的期望。  

• 右边：显式写出期望的积分形式，对全空间的所有 $x$ 求和（积分），权重为 $P_{data}(x)$。  


两者本质上是同一目标的不同写法，优化结果自然相同。

---

5. **补充：与最大似然估计的联系**  
这个等式实际上描述了 最大似然估计（MLE） 的核心思想：  
• 最大化 $E_{x \sim P_{\text{data}}} [\log P_\theta(x)]$ 等价于最小化真实分布 $P_{\text{data}}$ 和模型分布 $P_\theta$ 之间的交叉熵（Cross-Entropy）或 KL 散度。


---

总结  
等式成立的本质原因是：期望运算符 $E_{x \sim P}[\cdot]$ 的定义本身就是对概率分布的加权平均（积分或求和）。因此：  
$$
\arg\max_\theta E_{x \sim P_{\text{data}}} [\log P_\theta(x)] = \arg\max_\theta \int_{x} P_{\text{data}}(x) \log P_\theta(x) dx
$$

## 4.1 最大化期望与最小化交叉熵/ KL 散度的关系

为什么 最大化期望 $ E_{x \sim P_{\text{data}}}[\log P_\theta(x)] $ 等价于最小化交叉熵（Cross-Entropy）或 KL 散度?

---

1. **交叉熵（Cross-Entropy）的定义**
交叉熵衡量的是用模型分布 $ P_\theta(x) $ 编码来自真实分布 $ P_{\text{data}}(x) $ 的数据所需的平均信息量：
$$
H(P_{\text{data}}, P_\theta) = -E_{x \sim P_{\text{data}}}[\log P_\theta(x)].
$$
因此，最大化 $ E_{x \sim P_{\text{data}}}[\log P_\theta(x)] $ 等价于最小化交叉熵。

---

2. **KL 散度的定义**
KL 散度（Kullback-Leibler Divergence）衡量真实分布 $ P_{\text{data}} $ 和模型分布 $ P_\theta $ 之间的差异：
$$
D_{\text{KL}}(P_{\text{data}} \parallel P_\theta) = E_{x \sim P_{\text{data}}}\left[\log \frac{P_{\text{data}}(x)}{P_\theta(x)}\right].
$$
展开后：
$$
D_{\text{KL}} = \underbrace{E_{x \sim P_{\text{data}}}[\log P_{\text{data}}(x)]}_{-H(P_{\text{data}})} - E_{x \sim P_{\text{data}}}[\log P_\theta(x)].
$$
其中：
• $ H(P_{\text{data}}) $ 是真实分布的熵（固定值，与 $ \theta $ 无关）。


---

3. **交叉熵与 KL 散度的关系**
从 KL 散度的定义可推导出：
$$
D_{\text{KL}}(P_{\text{data}} \parallel P_\theta) = H(P_{\text{data}}, P_\theta) - H(P_{\text{data}}).
$$
由于 $ H(P_{\text{data}}) $ 是常数，最小化交叉熵 $ H(P_{\text{data}}, P_\theta) $ 等价于最小化 KL 散度。

---

4. **最终等价关系**
综合上述推导：
$$
\max_{\theta} E_{x \sim P_{\text{data}}}[\log P_\theta(x)] 
\quad \Leftrightarrow \quad 
\min_{\theta} H(P_{\text{data}}, P_\theta)
\quad \Leftrightarrow \quad 
\min_{\theta} D_{\text{KL}}(P_{\text{data}} \parallel P_\theta).
$$

---

5. **直观理解**
• 最大化对数似然：让模型 $ P_\theta(x) $ 对真实数据 $ x \sim P_{\text{data}} $ 赋予更高的概率。

• 最小化交叉熵：减少用 $ P_\theta(x) $ 编码真实数据所需的额外信息量。

• 最小化 KL 散度：迫使模型分布 $ P_\theta $ 逼近真实分布 $ P_{\text{data}} $。


---

6. **补充说明**
• 熵 $ H(P_{\text{data}}) $：表示数据本身的固有不确定性，与模型无关。

• 交叉熵 $ H(P_{\text{data}}, P_\theta) $：包含数据不确定性（$ H(P_{\text{data}}) $）和模型误差（$ D_{\text{KL}} $）。

• KL 散度非对称性：$ D_{\text{KL}}(P \parallel Q) \neq D_{\text{KL}}(Q \parallel P) $，这里目标是让 $ P_\theta $ 覆盖 $ P_{\text{data}} $。


---

总结
最大化对数似然 $ E_{x \sim P_{\text{data}}}[\log P_\theta(x)] $ 的本质是：  
通过最小化交叉熵或 KL 散度，使模型分布 $ P_\theta $ 尽可能接近真实分布 $ P_{\text{data}} $。  
这是最大似然估计（MLE）与信息论之间的深刻联系。

