# 1 FAN

## 1.1 理解

1.   FAN == DFT + RevIN：FAN在频域上利用傅里叶变换处理平稳与非平稳成分，而RevIN在时域上利用标准化处理。
1.   



### 关于X y的形状

**训练集**：

- `x_train.shape = (700, 96, 8)` → 700个样本，每个样本是 **96个时间步** × **8个特征**
- `y_train.shape = (700, 192, 8)` → 700个标签，每个标签是 **192个时间步** × **8个特征**

PyTorch 的 `DataLoader` 会将数据集按 `batch_size` 分成多个批次。每个批次的结构如下：

**训练集批次 (`train_loader`)**:

- **输入 `x_batch`**：`(batch_size, 96, 8)`
- **标签 `y_batch`**：`(batch_size, 192, 8)`

**测试集批次 (`test_loader`)**:

- **输入 `x_batch`**：`(batch_size, 96, 8)`
- **标签 `y_batch`**：`(batch_size, 192, 8)`

X：输入的 tensor，形状是 (B, T, N)，B: batch_size (批次大小), T: window (*输入序列长度*), N: num_features (特征数量)

y：targets, 也是 tensor, 形状是 (B, O, N), O: steps(*预测序列长度*)

实例级Fourier：对X在时间维度上做变换。比如形状是 (32, 12, 307)的X，就对所有32个样本的307个特征都分别进行变换。

全局级Fourier：也X在时间维度上做变换，但变换完后，在batch和特征维度上取幅值的平均值。另外，top_k固定(TimesNet)。

### FFT

在使用傅里叶变换（FFT）处理时间序列时，虽然变换后的形状与原始数据相同，但其**物理意义已完全不同**。以下是详细解释：

---

#### **1. FFT 的输入与输出**
**(1) 输入数据**

- 假设原始数据形状为 `(样本数, 通道数, 时间步数)`，例如 `(1000, 1, 500)`。
- 在代码中，先通过 `transpose(1, 2)` 将通道和时间维度交换，变为 `(样本数, 时间步数, 通道数)`，即 `(1000, 500, 1)`。
  - 这样做的目的是为了对每个通道（如传感器信号）独立进行 FFT。

**(2) FFT 的输出**

- 对长度为 $ N $ 的时间序列进行 FFT，结果是一个复数数组，包含 $ N $ 个频率分量。
  - 例如，原始时间序列有 500 个时间步，FFT 后仍为 500 个频率分量。
- 取绝对值后，得到幅度谱（Magnitude Spectrum），形状与输入一致。
  - 例如，输入形状为 `(1000, 500, 1)` → FFT 后形状仍为 `(1000, 500, 1)`。

**(3) 恢复维度**

- 最终通过 `transpose(1, 2)` 将通道和时间维度换回，形状恢复为 `(样本数, 通道数, 时间步数)`，即 `(1000, 1, 500)`。
  - 此时，每个时间步的值表示对应频率的幅度（而非原始时域值）。

---

#### **2. 形状不变的原因**
**(1) 频率分量与时间步一一对应**

- FFT 的本质是将时域信号分解为多个正弦/余弦波的叠加，每个时间步对应一个频率分量。
  - 例如，长度为 500 的时间序列 → 500 个频率分量。
- 因此，FFT 的输出长度与输入长度相同，形状保持一致。

**(2) 保留时间序列结构**

- 通过保持形状一致，可以直接将频域特征作为模型输入，无需额外调整网络结构。
  - 例如，卷积神经网络（CNN）可以像处理时域信号一样处理频域信号。

**(3) 多通道独立处理**

- 每个通道（如多个传感器）独立进行 FFT，因此通道维度不受影响。

---

#### **3. 关键代码逻辑**
```python
# 1. 转置以在时间维度上做 FFT
train_X_transposed = train_X.transpose(1, 2)  # (样本数, 时间步数, 通道数)

# 2. FFT 变换并取绝对值
train_X_fft = torch.fft.fft(train_X_transposed).abs()  # (样本数, 时间步数, 通道数)

# 3. 恢复通道和时间维度
train_X_fft = train_X_fft.transpose(1, 2)  # (样本数, 通道数, 时间步数)
```



## 1.2 疑问

1.  为什么采用MLP来处理非平稳的信号？考虑到RNN捕获时间依赖信息的能力更强，为什么FAN没有考虑用LSTM、GRU？
   1.  实验结果显示3层MLP的效果最好，可能的原因是“主频率信号为骨干模型提供了基线位置，从而导致更稳健的预测”
   2.  在所有基准测试中，主频率分量的变化小于残差的变化。这就是为什么一个简单的MLP足够有效地捕捉到主要频率变化的原因。（它的偏移相对较小）

2.  为什么数据中的低频部分表示趋势，而高频部分表示季节性(seasonal)
    1.  **频谱角度的解释：**
         当我们将一个时间序列进行频谱分析时，可以看到不同频率的成分在信号中所占比例。低频成分代表的是那些周期较长、缓慢变化的部分（趋势），而高频成分则代表周期较短、振幅较大的变化（季节性波动）。
    2.  **数据模型的适用性：**
         如果一个数据集中不仅有显著的低频趋势，还存在明显的高频季节性波动，则一个能够同时捕捉两种信息的模型（例如文中提到的方法）可能会在预测性能上有显著提升。这也是为什么在高频变化（即季节性变化）明显的数据集上，模型表现提升更为明显（如最高提升达到了 19.90%、18.65% 等）。

