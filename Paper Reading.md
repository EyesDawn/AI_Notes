# 1 FAN

## 1.1 理解

1.   FAN == DFT + RevIN：FAN在频域上利用傅里叶变换处理平稳与非平稳成分，而RevIN在时域上利用标准化处理。
1.   



### 关于X y的形状

**训练集**：

- `x_train.shape = (700, 96, 8)` → 700个样本，每个样本是 **96个时间步** × **8个特征**
- `y_train.shape = (700, 192, 8)` → 700个标签，每个标签是 **192个时间步** × **8个特征**

PyTorch 的 `DataLoader` 会将数据集按 `batch_size` 分成多个批次。每个批次的结构如下：

**训练集批次 (`train_loader`)**:

- **输入 `x_batch`**：`(batch_size, 96, 8)`
- **标签 `y_batch`**：`(batch_size, 192, 8)`

**测试集批次 (`test_loader`)**:

- **输入 `x_batch`**：`(batch_size, 96, 8)`
- **标签 `y_batch`**：`(batch_size, 192, 8)`

X：输入的 tensor，形状是 (B, T, N)，B: batch_size (批次大小), T: window (*输入序列长度*), N: num_features (特征数量)

y：targets, 也是 tensor, 形状是 (B, O, N), O: steps(*预测序列长度*)

实例级Fourier：对X在时间维度上做变换。比如形状是 (32, 12, 307)的X，就对所有32个样本的307个特征都分别进行变换。

全局级Fourier：也X在时间维度上做变换，但变换完后，在batch和特征维度上取幅值的平均值。另外，top_k固定(TimesNet)。

### FFT

在使用傅里叶变换（FFT）处理时间序列时，虽然变换后的形状与原始数据相同，但其**物理意义已完全不同**。以下是详细解释：

---

#### **1. FFT 的输入与输出**
**(1) 输入数据**

- 假设原始数据形状为 `(样本数, 通道数, 时间步数)`，例如 `(1000, 1, 500)`。
- 在代码中，先通过 `transpose(1, 2)` 将通道和时间维度交换，变为 `(样本数, 时间步数, 通道数)`，即 `(1000, 500, 1)`。
  - 这样做的目的是为了对每个通道（如传感器信号）独立进行 FFT。

**(2) FFT 的输出**

- 对长度为 $ N $ 的时间序列进行 FFT，结果是一个复数数组，包含 $ N $ 个频率分量。
  - 例如，原始时间序列有 500 个时间步，FFT 后仍为 500 个频率分量。
- 取绝对值后，得到幅度谱（Magnitude Spectrum），形状与输入一致。
  - 例如，输入形状为 `(1000, 500, 1)` → FFT 后形状仍为 `(1000, 500, 1)`。

**(3) 恢复维度**

- 最终通过 `transpose(1, 2)` 将通道和时间维度换回，形状恢复为 `(样本数, 通道数, 时间步数)`，即 `(1000, 1, 500)`。
  - 此时，每个时间步的值表示对应频率的幅度（而非原始时域值）。

---

#### **2. 形状不变的原因**
**(1) 频率分量与时间步一一对应**

- FFT 的本质是将时域信号分解为多个正弦/余弦波的叠加，每个时间步对应一个频率分量。
  - 例如，长度为 500 的时间序列 → 500 个频率分量。
- 因此，FFT 的输出长度与输入长度相同，形状保持一致。

**(2) 保留时间序列结构**

- 通过保持形状一致，可以直接将频域特征作为模型输入，无需额外调整网络结构。
  - 例如，卷积神经网络（CNN）可以像处理时域信号一样处理频域信号。

**(3) 多通道独立处理**

- 每个通道（如多个传感器）独立进行 FFT，因此通道维度不受影响。

---

#### **3. 关键代码逻辑**
```python
# 1. 转置以在时间维度上做 FFT
train_X_transposed = train_X.transpose(1, 2)  # (样本数, 时间步数, 通道数)

# 2. FFT 变换并取绝对值
train_X_fft = torch.fft.fft(train_X_transposed).abs()  # (样本数, 时间步数, 通道数)

# 3. 恢复通道和时间维度
train_X_fft = train_X_fft.transpose(1, 2)  # (样本数, 通道数, 时间步数)
```



## 1.2 疑问

1.  为什么采用MLP来处理非平稳的信号？考虑到RNN捕获时间依赖信息的能力更强，为什么FAN没有考虑用LSTM、GRU？
   1.  实验结果显示3层MLP的效果最好，可能的原因是“主频率信号为骨干模型提供了基线位置，从而导致更稳健的预测”
   2.  在所有基准测试中，主频率分量的变化小于残差的变化。这就是为什么一个简单的MLP足够有效地捕捉到主要频率变化的原因。（它的偏移相对较小）

2.  为什么数据中的低频部分表示趋势，而高频部分表示季节性(seasonal)
    1.  **频谱角度的解释：**
         当我们将一个时间序列进行频谱分析时，可以看到不同频率的成分在信号中所占比例。低频成分代表的是那些周期较长、缓慢变化的部分（趋势），而高频成分则代表周期较短、振幅较大的变化（季节性波动）。
    2.  **数据模型的适用性：**
         如果一个数据集中不仅有显著的低频趋势，还存在明显的高频季节性波动，则一个能够同时捕捉两种信息的模型（例如文中提到的方法）可能会在预测性能上有显著提升。这也是为什么在高频变化（即季节性变化）明显的数据集上，模型表现提升更为明显（如最高提升达到了 19.90%、18.65% 等）。

# 2 The Rise of Diffusion Models in Time-Series Forecasting

## 2.1 Evaluation Metrics

$$
\mathrm{MSE}(\hat{X}_{tar},X_{tar})=\frac1F\sum_{t=0}^F(x_t-\hat{x}_t)^2\quad\mathrm{MAE}(\hat{X}_{tar},X_{tar})=\frac1F\sum_{t=0}^F|x_t-\hat{x}_t|
$$

$$
\mathrm{CRPS}(\hat{F}_{i,t}^N,x_{i,t})=\int_{\mathbb{R}}\left(\hat{F}_{i,t}^N(z)-\mathbb{I}\{x_{i,t}\leq z\}\right)^2dz
$$

MSE和MAE关注的是平均误差，是**一个点预测值**和实际值之间的距离；而CRPS评估**概率预测的好坏** ，是预测的分布与阶跃分布（实际结果）之间的差距面积。

## 2.2 Diffusion Models

**扩散模型通过“加噪-去噪”的过程训练生成器模型**，通过 KL 散度或噪声预测损失来训练神经网络，最终实现“从噪声中生成高质量数据”的目标。

### 2.2.1 KL散度损失 

我们希望学习的分布 $p_\theta(x^{k-1}|x^k)$ 能够尽量接近真实的后验分布 $q(x^{k-1}|x^k)$

### 2.2.2 重参数化目标

通过推导，可以把损失变成如下形式：
$$
\mathcal{L}_k = \frac{1}{2\sigma_k^2} \|\tilde{\mu}_k(x^k, x^0) - \mu_\theta(x^k, k)\|^2
$$

- $\tilde{\mu}_k$ 是基于 $x^0$ 和 $x^k$ 计算的真实后验均值
- $\mu_\theta$ 是模型预测的去噪均值

直观来说：模型学习“如何从噪声图像猜出原图”

Ho 等人提出，直接预测噪声（而不是图像）效果更好：
$$
\mu_\theta(\epsilon_\theta) = \frac{1}{\sqrt{1 - \hat{\beta}_k}} \left( x^k - \frac{\beta_k}{\sqrt{1 - \alpha_k}} \epsilon_\theta(x^k, k) \right)
$$
最终损失就是预测噪声和真实噪声的差距：
$$
\mathcal{L}_\epsilon = \mathbb{E}_{x,k,\epsilon} \left[ \|\epsilon - \epsilon_\theta(x^k, k)\|^2 \right]
$$

### 2.2.3 Score-based generative modeling through SDE

扩散模型的目标是：从随机噪声中一步步“反推出”真实数据。为了让这个过程更精细和灵活，有人提出可以用 **连续时间的数学模型：随机微分方程（SDE）** 来描述这个“加噪声”和“去噪声”的过程。

#### 📘 公式 (18)：正向扩散过程 SDE

$$
dx = f(x, k)\,dk + g(k)\,d\mathbf{w}
$$

这个公式描述的是：**你怎么往真实数据里“注入噪声”**。

- $x$：当前的数据（图像、音频等）
- $k$：当前的“时间步”，在 [0, K] 之间（从干净到全噪声）
- $f(x,k)\,dk$：表示“自然演化”（可以是让图像逐渐模糊的趋势）
- $g(k)\,d\mathbf{w}$：表示加入的随机噪声

这就像：你每秒都往图像里加入一些模糊、抖动、随机扰动。

------

#### 🔁 公式 (19)：反向去噪过程 SDE

$$
dx = \left[f(x, k) - g(k)^2 \nabla_x \log p_k(x) \right]\,dk + g(k)\,d\bar{\mathbf{w}}
$$

这是“反过来”的过程 —— **从噪声变成图像**

- $\nabla_x \log p_k(x)$：这个是关键，它叫做 **score**，表示“在当前图像附近，什么方向更可能是干净图像”
- 整个式子本质是：“沿着得分的方向”一步步去除噪声

------

#### 🧠 什么是 score（$\nabla_x \log p(x)$）？

它可以看作是 **“指向真实数据的方向”**。

在一个图像加了噪声后，我们想知道：“往哪个方向走，能让它看起来更像一个真实图像？”
 这就是 score 函数告诉你的东西。

------

#### 🧪 公式 (20)：训练目标（score matching）

$$
\mathcal{L}_{s_\theta} = \mathbb{E} \left[\| \nabla_x \log p_k(x) - s_\theta(x, k) \|^2 \right]
$$

你训练一个神经网络 $s_\theta(x, k)$，让它学会“预测 score”，也就是预测“我该往哪个方向去掉噪声”。

这相当于训练一个“方向指引器”。

------

#### 🧭 ODE 和 “概率流” 是什么？

在去噪的过程中，原来的公式有**随机扰动项**，也就是 $d\bar{\mathbf{w}}$。这意味着：

- 每次生成图像的过程可能不同
- 模型不容易精确控制

于是我们引入一个“确定性版本”：

------

#### 📘 公式 (23)：概率流 ODE

$$
dx = \left[f(x, k) - \frac{1}{2}g(k)^2 \nabla_x \log p_k(x) \right] dk
$$

和公式 (19) 类似，但是**没有随机扰动项**，也就是：

- 每次生成的结果是完全确定的
- 更像“常微分方程”（ODE）而不是随机过程
- 用起来可以更稳定、更易训练，适合做最大似然估计

#### ✅ 总结一句话：

> 公式 (18)-(23) 是在告诉你，扩散模型可以用“连续时间的数学语言（SDE/ODE）”来描述和实现。
>  正向是不断加噪，反向是沿着得分方向去噪。为了更可控，还可以用概率流 ODE 实现一个**无噪声的确定性生成过程**。
