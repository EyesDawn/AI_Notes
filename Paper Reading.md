# 1 FAN

## 1.1 理解

1.   FAN == DFT + RevIN：FAN在频域上利用傅里叶变换处理平稳与非平稳成分，而RevIN在时域上利用标准化处理。
1.   



### 关于X y的形状

**训练集**：

- `x_train.shape = (700, 96, 8)` → 700个样本，每个样本是 **96个时间步** × **8个特征**
- `y_train.shape = (700, 192, 8)` → 700个标签，每个标签是 **192个时间步** × **8个特征**

PyTorch 的 `DataLoader` 会将数据集按 `batch_size` 分成多个批次。每个批次的结构如下：

**训练集批次 (`train_loader`)**:

- **输入 `x_batch`**：`(batch_size, 96, 8)`
- **标签 `y_batch`**：`(batch_size, 192, 8)`

**测试集批次 (`test_loader`)**:

- **输入 `x_batch`**：`(batch_size, 96, 8)`
- **标签 `y_batch`**：`(batch_size, 192, 8)`

X：输入的 tensor，形状是 (B, T, N)，B: batch_size (批次大小), T: window (*输入序列长度*), N: num_features (特征数量)

y：targets, 也是 tensor, 形状是 (B, O, N), O: steps(*预测序列长度*)

实例级Fourier：对X在时间维度上做变换。比如形状是 (32, 12, 307)的X，就对所有32个样本的307个特征都分别进行变换。

全局级Fourier：也X在时间维度上做变换，但变换完后，在batch和特征维度上取幅值的平均值。另外，top_k固定(TimesNet)。

### FFT

在使用傅里叶变换（FFT）处理时间序列时，虽然变换后的形状与原始数据相同，但其**物理意义已完全不同**。以下是详细解释：

---

#### **1. FFT 的输入与输出**
**(1) 输入数据**

- 假设原始数据形状为 `(样本数, 通道数, 时间步数)`，例如 `(1000, 1, 500)`。
- 在代码中，先通过 `transpose(1, 2)` 将通道和时间维度交换，变为 `(样本数, 时间步数, 通道数)`，即 `(1000, 500, 1)`。
  - 这样做的目的是为了对每个通道（如传感器信号）独立进行 FFT。

**(2) FFT 的输出**

- 对长度为 $ N $ 的时间序列进行 FFT，结果是一个复数数组，包含 $ N $ 个频率分量。
  - 例如，原始时间序列有 500 个时间步，FFT 后仍为 500 个频率分量。
- 取绝对值后，得到幅度谱（Magnitude Spectrum），形状与输入一致。
  - 例如，输入形状为 `(1000, 500, 1)` → FFT 后形状仍为 `(1000, 500, 1)`。

**(3) 恢复维度**

- 最终通过 `transpose(1, 2)` 将通道和时间维度换回，形状恢复为 `(样本数, 通道数, 时间步数)`，即 `(1000, 1, 500)`。
  - 此时，每个时间步的值表示对应频率的幅度（而非原始时域值）。

---

#### **2. 形状不变的原因**
**(1) 频率分量与时间步一一对应**

- FFT 的本质是将时域信号分解为多个正弦/余弦波的叠加，每个时间步对应一个频率分量。
  - 例如，长度为 500 的时间序列 → 500 个频率分量。
- 因此，FFT 的输出长度与输入长度相同，形状保持一致。

**(2) 保留时间序列结构**

- 通过保持形状一致，可以直接将频域特征作为模型输入，无需额外调整网络结构。
  - 例如，卷积神经网络（CNN）可以像处理时域信号一样处理频域信号。

**(3) 多通道独立处理**

- 每个通道（如多个传感器）独立进行 FFT，因此通道维度不受影响。

---

#### **3. 关键代码逻辑**
```python
# 1. 转置以在时间维度上做 FFT
train_X_transposed = train_X.transpose(1, 2)  # (样本数, 时间步数, 通道数)

# 2. FFT 变换并取绝对值
train_X_fft = torch.fft.fft(train_X_transposed).abs()  # (样本数, 时间步数, 通道数)

# 3. 恢复通道和时间维度
train_X_fft = train_X_fft.transpose(1, 2)  # (样本数, 通道数, 时间步数)
```



## 1.2 疑问

1.  为什么采用MLP来处理非平稳的信号？考虑到RNN捕获时间依赖信息的能力更强，为什么FAN没有考虑用LSTM、GRU？
   1.  实验结果显示3层MLP的效果最好，可能的原因是“主频率信号为骨干模型提供了基线位置，从而导致更稳健的预测”
   2.  在所有基准测试中，主频率分量的变化小于残差的变化。这就是为什么一个简单的MLP足够有效地捕捉到主要频率变化的原因。（它的偏移相对较小）

2.  为什么数据中的低频部分表示趋势，而高频部分表示季节性(seasonal)
    1.  **频谱角度的解释：**
         当我们将一个时间序列进行频谱分析时，可以看到不同频率的成分在信号中所占比例。低频成分代表的是那些周期较长、缓慢变化的部分（趋势），而高频成分则代表周期较短、振幅较大的变化（季节性波动）。
    2.  **数据模型的适用性：**
         如果一个数据集中不仅有显著的低频趋势，还存在明显的高频季节性波动，则一个能够同时捕捉两种信息的模型（例如文中提到的方法）可能会在预测性能上有显著提升。这也是为什么在高频变化（即季节性变化）明显的数据集上，模型表现提升更为明显（如最高提升达到了 19.90%、18.65% 等）。

# 2 The Rise of Diffusion Models in Time-Series Forecasting

## 2.1 Evaluation Metrics

$$
\mathrm{MSE}(\hat{X}_{tar},X_{tar})=\frac1F\sum_{t=0}^F(x_t-\hat{x}_t)^2\quad\mathrm{MAE}(\hat{X}_{tar},X_{tar})=\frac1F\sum_{t=0}^F|x_t-\hat{x}_t|
$$

$$
\mathrm{CRPS}(\hat{F}_{i,t}^N,x_{i,t})=\int_{\mathbb{R}}\left(\hat{F}_{i,t}^N(z)-\mathbb{I}\{x_{i,t}\leq z\}\right)^2dz
$$

MSE和MAE关注的是平均误差，是**一个点预测值**和实际值之间的距离；而CRPS评估**概率预测的好坏** ，是预测的分布与阶跃分布（实际结果）之间的差距面积。

## 2.2 Diffusion Models

**扩散模型通过“加噪-去噪”的过程训练生成器模型**，通过 KL 散度或噪声预测损失来训练神经网络，最终实现“从噪声中生成高质量数据”的目标。

### 2.2.1 Denoising Diffusion Probabilistic Model (DDPM)

#### 📈 正向过程（Forward Process）公式 (6)-(8)

##### 1. 公式 (6)：加噪声的马尔可夫过程

$$
q(x^{1:K}|x^0) = \prod_{k=1}^{K} q(x^k | x^{k-1})\quad \text{且 } q(x^k | x^{k-1}) = \mathcal{N}(x^k; \sqrt{1 - \beta_k}x^{k-1}, \beta_k I)
$$

表示：每一步用一个高斯噪声扰动前一步的结果。

- $\beta_k$：第 k 步的噪声强度
- $\mathcal{N}(\cdot, \cdot)$：高斯分布

------

##### 2. 公式 (7)-(8)：一步计算任意时刻的结果

你可以从初始数据 $x^0$ 一步跳到任意时间步 $x^k$，不需要按步骤模拟：
$$
x^k = \sqrt{\alpha_k} x^0 + \sqrt{1 - \alpha_k} \cdot \epsilon,\quad \epsilon \sim \mathcal{N}(0, I)
$$

- 这里 $\alpha_k = \prod_{i=1}^{k} (1 - \beta_i)$
- 实际意思：噪声成分越来越多，$x^k$ 越来越“模糊”

------

#### 🔁 反向过程（Reverse Process）公式 (9)-(10)

反向过程是模型学习的目标：从纯噪声一步步“还原”出干净数据。
$$
p_\theta(x^{0:K}) = p(x^K) \prod_{k=1}^K p_\theta(x^{k-1}|x^k)
$$
其中：

- $x^{0:K}$: 表示序列 ($x^0, x^1, ..., x^K$), 其中:
    - $x^0$: 原始数据 (如图像、音频等)。
    - $x^k$ ($k = 1, ..., K$): 添加噪声后的中间状态, $x^K$ 是完全的纯噪声。
- $p(x^K)$: 噪声的先验分布, 通常是标准高斯分布 $\mathcal{N}(0, I)$。它描述扩散过程结束时的噪声状态。
- $p_\theta(x^{k-1}|x^k)$: 参数化的逆向转移概率。给定当前噪声状态 $x^k$, 它预测前一步的“去噪”状态 $x^{k-1}$。这是由神经网络 (如U-Net) 参数化的, 其中 $\theta$ 是模型的可学习参数。
- $\prod_{k=1}^{K}$: 表示逆向过程是一个马尔可夫链, 每一步仅依赖前一步的状态。

- 每一步：$p_\theta(x^{k-1}|x^k) = \mathcal{N}(x^{k-1}; \mu_\theta(x^k, k), \sigma_k^2 I)$

$\mu_\theta(x^k, k)$ 是通过神经网络预测的均值（代表我们“去噪”后的猜测）

------

##### 公式 (10)：可选择的方差

$$
\sigma_k^2 = 
\begin{cases}
\frac{\beta_k}{\hat{\beta}_k}, & \text{如果 } x^0 \sim \mathcal{N}(0, I) \\
\frac{1 - \alpha_{k-1}}{1 - \alpha_k}, & \text{如果 } x^0 是确定的
\end{cases}
$$

#### 🧠训练目标（公式 11-15）

##### 1. KL散度损失(11)

我们希望学习的分布 $p_\theta(x^{k-1}|x^k)$ 能够尽量接近真实的后验分布 $q(x^{k-1}|x^k)$

##### 2. 重参数化目标(12)-(15)

通过推导，可以把损失变成如下形式：
$$
\mathcal{L}_k = \frac{1}{2\sigma_k^2} \|\tilde{\mu}_k(x^k, x^0) - \mu_\theta(x^k, k)\|^2
$$

- $\tilde{\mu}_k$ 是基于 $x^0$ 和 $x^k$ 计算的真实后验均值
- $\mu_\theta$ 是模型预测的去噪均值

直观来说：模型学习“如何从噪声图像猜出原图”

Ho 等人提出，直接预测噪声（而不是图像）效果更好：
$$
\mu_\theta(\epsilon_\theta) = \frac{1}{\sqrt{1 - \hat{\beta}_k}} \left( x^k - \frac{\beta_k}{\sqrt{1 - \alpha_k}} \epsilon_\theta(x^k, k) \right)
$$
最终损失就是预测噪声和真实噪声的差距：
$$
\mathcal{L}_\epsilon = \mathbb{E}_{x,k,\epsilon} \left[ \|\epsilon - \epsilon_\theta(x^k, k)\|^2 \right]
$$

### 2.2.2 Score-based generative modeling through SDE

扩散模型的目标是：从随机噪声中一步步“反推出”真实数据。为了让这个过程更精细和灵活，有人提出可以用 **连续时间的数学模型：随机微分方程（SDE）** 来描述这个“加噪声”和“去噪声”的过程。

#### 📘 公式 (18)：正向扩散过程 SDE

$$
dx = f(x, k)\,dk + g(k)\,d\mathbf{w}
$$

这个公式描述的是：**你怎么往真实数据里“注入噪声”**。

- $x$：当前的数据（图像、音频等）。$dx$ 描述的是数据状态在一个小时间段内的变化。
- $k$：当前的“时间步”，在 [0, K] 之间（从干净到全噪声）。$dk$则是时间步的变化量。
- $f(x,k)\,dk$：表示“自然演化”（可以是让图像逐渐模糊的趋势）
- $g(k)\,d\mathbf{w}$：表示加入的随机噪声，其中 **$g(k)$** 是与当前时间步 $k$ 相关的噪声强度，**$d\mathbf{w}$** 是一个标准布朗运动增量（随机变化的部分）。

这就像：你每秒都往图像里加入一些模糊、抖动、随机扰动。

------

#### 🔁 公式 (19)：反向去噪过程 SDE

$$
dx = \left[f(x, k) - g(k)^2 \nabla_x \log p_k(x) \right]\,dk + g(k)\,d\bar{\mathbf{w}}
$$

这是“反过来”的过程 —— **从噪声变成图像**

- $\nabla_x \log p_k(x)$：这个是关键，它叫做 **score**，表示“在当前图像附近，什么方向更可能是干净图像”
- **$g(k)\,d\bar{\mathbf{w}}$**：和正向扩散过程中的噪声项类似，表示噪声在反向去噪过程中加入的部分
- 整个式子本质是：“沿着得分的方向”一步步去除噪声

------

#### 🧠 什么是 score（$\nabla_x \log p(x)$）？

它表示 **在当前图像附近的梯度方向**，可以看作是 **“指向真实数据的方向”**。

在一个图像加了噪声后，我们想知道：“往哪个方向走，能让它看起来更像一个真实图像？”
 这就是 score 函数告诉你的东西。

------

#### 🧪 公式 (20)：训练目标（score matching）

$$
\mathcal{L}_{s_\theta} = \mathbb{E} \left[\| \nabla_x \log p_k(x) - s_\theta(x, k) \|^2 \right]
$$

你训练一个神经网络 $s_\theta(x, k)$，让它学会“预测 score”，也就是预测“我该往哪个方向去掉噪声”。

这相当于训练一个“方向指引器”。

------

#### 🧭 ODE 和 “概率流” 是什么？

在去噪的过程中，原来的公式有**随机扰动项**，也就是 $d\bar{\mathbf{w}}$。这意味着：

- 每次生成图像的过程可能不同
- 模型不容易精确控制

于是我们引入一个“确定性版本”：

------

#### 📘 公式 (23)：概率流 ODE

$$
dx = \left[f(x, k) - \frac{1}{2}g(k)^2 \nabla_x \log p_k(x) \right] dk
$$

和公式 (19) 类似，但是**没有随机扰动项**，也就是：

- 每次生成的结果是完全确定的
- 更像“常微分方程”（ODE）而不是随机过程
- 用起来可以更稳定、更易训练，适合做最大似然估计

#### ✅ 总结一句话：

> 公式 (18)-(23) 是在告诉你，扩散模型可以用“连续时间的数学语言（SDE/ODE）”来描述和实现。
>  正向是不断加噪，反向是沿着得分方向去噪。为了更可控，还可以用概率流 ODE 实现一个**无噪声的确定性生成过程**。

### 2.2.3 Conditional Diffusion

#### ✨ 第一种方式：**条件建模 during 训练阶段**（图 Figure 8）

这其实和普通去噪过程类似，只不过我们在模型输入里**加上了条件 `c`**。

**对应公式：**
$$
p_\theta(x^{k-1}|x^k, c) = \mathcal{N}(x^{k-1}; \mu_\theta(x^k, k, c), \sigma_k^2 I)
$$
和之前的反向过程几乎一样，只是现在每一步都带上了条件 `c`。

**模型预测的方式也被“加了条件”：**

- 噪声预测方式：
  $$
  \mu_\epsilon(\epsilon_\theta, c)
  $$

- 原图预测方式：
  $$
  \mu_x(x_\theta, c)
  $$

这些公式都和之前一样，只不过神经网络接收了额外的条件输入 `c`，图 Figure 8 也说明了这一点。

------

#### 🧲 第二种方式：**Diffusion Guidance during 推理阶段**（图 Figure 9）

这是一种更灵活、更聪明的方法：**在推理（采样）的时候再引导模型往“目标方向”生成**。

也就是：

- 模型原来训练的时候没带条件
- 但现在我可以在采样的时候，用**贝叶斯规则**引导它往想要的样子靠近！

------

**用贝叶斯公式重新写条件概率：**
$$
p(x^k | c) \propto p(c | x^k) \cdot p(x^k)
$$
取对数后，推导出“引导梯度”：
$$
\nabla_{x^k} \log p(x^k | c) = \nabla_{x^k} \log p(c | x^k) + \nabla_{x^k} \log p(x^k)
$$
这个导数就是“往满足条件的方向微调”的方式。

------

✨ **最终采样方式变成**（公式 32）：
$$
p_\theta(x^{k-1}|x^k) = \mathcal{N}\left(\mu_\theta(x^k, k), \sigma_k^2 I\right) + s\sigma_k^2 \nabla_{x^k} \log p(c | x^k)
$$
红框里加了一项：

- $\nabla_{x^k} \log p(c | x^k)$：**这个告诉你：怎么让生成结果更像你要的条件**
- $s$：引导强度（可以控制“像条件”还是“保持自然”）

## 2.3 Related Papers

### 2.3.1 TimeGrad

Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (2021) 

#### 🧠什么是 Autoregressive？（自回归）

**Autoregressive（AR，自回归）** 是时间序列建模里很经典的概念：

> 一个模型是“自回归的”，意思是：
>  **当前时刻的输出依赖于前面已经预测出来的结果。**

#### 🔍 在 TimeGrad 中“Autoregressive”的体现

TimeGrad 预测的是未来的多步值（例如未来 24 步的电力需求），它没有一次性同时输出全部，而是**一步一步地预测：**
$$
x_1 \rightarrow x_2 \rightarrow x_3 \rightarrow \cdots \rightarrow x_F
$$

------

🧱 **对应公式位置在哪**？

✅ 公式 (36)：
$$
p_\theta(X^{0:K}_{tar} \mid X_{obs}) = \prod_{t=1}^F p_\theta(x^{0:K}_t \mid h_{t-1})
$$
这句话的意思是：**第 $t$ 个未来值的预测，是依赖于前 $t-1$ 步的信息的。**

而 $h_{t-1}$ 是通过 RNN 来编码前面历史+预测结果得到的隐藏状态。

------

✅ 公式 (35)：
$$
h_t = \text{RNN}_\theta(\text{concat}(x^v_t, c_t), h_{t-1})
$$

- RNN 本质上就是一种自回归结构，因为每一步的输出都要用上前一步的隐藏状态。
- 在 TimeGrad 里，这个隐藏状态 $h_{t-1}$ 会影响下一个时间点的预测。

#### ✅ 总结一句话：

> **TimeGrad 之所以叫 “Autoregressive”，是因为它是一步一步预测未来的时间点，每一步都依赖于前面已经预测的内容，这种结构就叫自回归。**
>  它的“自回归性”体现在：使用 RNN 的隐藏状态 $h_{t-1}$ 来预测每一个时间步，并且每一个未来点是按顺序、递推地预测出来的。

### 2.3.2 ScoreGrad

Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models (2021)

#### ✅ 小结一下：

| 问题                   | 回答                                                         |
| ---------------------- | ------------------------------------------------------------ |
| ScoreGrad 改进了什么？ | 从离散DDPM换成连续SDE，更灵活、性能更好                      |
| 有什么新符号？         | $\mathbf{F}_t$ 替代 $h_t$ 表示历史信息，score预测用 $s_\theta$ |
| 模型结构是怎样的？     | 仍然是自回归结构，一步预测一步，适用于多变量时间序列         |
| 表格说明了什么？       | ScoreGrad 尤其是 sub-VP SDE 效果更好，尤其在电力数据集上表现极好 |

### 2.3.3 CSDI

Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation (2021)

#### 📌 与 TimeGrad / ScoreGrad 的不同点？

| 项目               | TimeGrad / ScoreGrad          | CSDI                                   |
| ------------------ | ----------------------------- | -------------------------------------- |
| 任务               | 预测未来时间点                | 补全任意缺失值（预测过去、现在或未来） |
| 条件信息编码       | 用 RNN 或其他模型编码历史信息 | 用 Transformer 编码条件位置            |
| 是否按时间顺序生成 | 是（autoregressive）          | 否，可以并行补全多个位置               |
| 特点               | 适合 forecasting              | 适合 imputation（带 mask）             |
